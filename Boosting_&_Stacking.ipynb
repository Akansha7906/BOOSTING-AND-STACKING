{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question** **1**:   What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.?"
      ],
      "metadata": {
        "id": "V2zVqlBJI98U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1**.  Boosting is an ensemble learning method in machine learning that combines several weak learners to form a strong learner with high accuracy\n",
        "\n",
        "\n",
        "**2** .A weak learner is a model that performs slightly better than random guessing, like a small decision tree or decision stump\n",
        "\n",
        "\n",
        "**3** In boosting, weak models are trained one after another, and each new model focuses more on the samples that were misclassified by the previous model\n",
        "\n",
        "\n",
        "**4**  Initially, all data points are given equal weights, and after each round, the weights of the wrongly classified samples are increased.\n",
        "5.This makes the next model pay more attention to difficult cases that were not correctly predicted before\n",
        "\n",
        "\n",
        "**6** The final strong model is obtained by combining all the weak learners through weighted voting (for classification) or weighted averaging (for regression)\n",
        "\n",
        "\n",
        "**7** Boosting helps in reducing both bias and variance, leading to improved model performance and better generalization on unseen data\n",
        "\n",
        "\n",
        "**8** The technique improves weak learners by making them learn from previous mistakes and by assigning higher weights to more accurate models\n",
        "\n",
        "\n",
        "**9**  Some common boosting algorithms are AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost\n",
        "\n",
        "\n",
        "**10** For example, if a single weak decision tree gives 70% accuracy, boosting can combine many such weak trees to achieve around 90–95% accuracy"
      ],
      "metadata": {
        "id": "Vz89q7ZqJpKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question** **2**: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n"
      ],
      "metadata": {
        "id": "zoDoEVy7KUk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1** AdaBoost (Adaptive Boosting) and Gradient Boosting are both boosting algorithms, but they differ in the way they train models and update errors\n",
        "\n",
        "\n",
        "**2** In AdaBoost, each weak learner is trained sequentially, and after every iteration, the weights of misclassified samples are increased so that the next model focuses more on those difficult cases\n",
        "\n",
        "\n",
        "**3** AdaBoost assigns higher weights to wrongly predicted data points and lower weights to correctly predicted ones\n",
        "\n",
        "\n",
        "**4** It combines the weak learners by giving each model a weight based on its accuracy, and the final prediction is made using weighted voting (for classification) or weighted average (for regression)\n",
        "\n",
        "\n",
        "**5** In Gradient Boosting, instead of adjusting sample weights, each new model is trained to minimize the residual errors (difference between actual and predicted values) from the previous model\n",
        "\n",
        "\n",
        "**6**  It uses the concept of gradient descent to reduce the loss function step by step\n",
        "\n",
        "\n",
        "**7** While AdaBoost focuses on reweighting data samples, Gradient Boosting focuses on reducing prediction errors using gradients\n",
        "\n",
        "\n",
        "**8**  AdaBoost is more sensitive to noisy data and outliers because it keeps increasing their weights, whereas Gradient Boosting is comparatively more robust\n",
        "\n",
        "\n",
        "**9** In simple terms, AdaBoost improves performance by changing the importance (weights) of data points, while Gradient Boosting improves performance by optimizing the loss function through gradient updates\n",
        "\n",
        "\n",
        "**10** Both aim to convert weak learners into a strong learner, but their training approach and error correction methods are different."
      ],
      "metadata": {
        "id": "AJsvJ1v3KrCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question** **3**: How does regularization help in XGBoost?"
      ],
      "metadata": {
        "id": "ZUubf5y2Libo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1**  Regularization in XGBoost helps to prevent the model from overfitting by adding a penalty for complex models\n",
        "\n",
        "\n",
        "**2**  It controls the model’s complexity by penalizing trees that have too many leaves or high weights\n",
        "\n",
        "\n",
        "**3**  XGBoost includes two regularization terms — L1 (Lasso) and L2 (Ridge) — in its objective function\n",
        "\n",
        "\n",
        "**4**  The L1 regularization (Lasso) helps by making some feature weights zero, which performs feature selection and simplifies the model\n",
        "\n",
        "\n",
        "**5** The L2 regularization (Ridge) helps by reducing the magnitude of large weights, leading to a smoother and more stable model\n",
        "\n",
        "\n",
        "**6** By applying these penalties, XGBoost reduces the chance of fitting noise in the training data\n",
        "\n",
        "\n",
        "**7**  Regularization also improves the model’s generalization ability, helping it perform better on unseen data\n",
        "\n",
        "\n",
        "**8**  It balances the trade-off between model accuracy and complexity, keeping the model both powerful and simple\n",
        "\n",
        "\n",
        "**9**  With regularization, XGBoost avoids creating overly deep or complex trees that might memorize the training data\n",
        "\n",
        "\n",
        "**10**  Hence, regularization in XGBoost ensures better performance, stability, and higher prediction accuracy on test data."
      ],
      "metadata": {
        "id": "TkaNNKjTLn6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question** **4**: Why is CatBoost considered efficient for handling categorical data?"
      ],
      "metadata": {
        "id": "HNcPTSMsMKo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1**  CatBoost is considered highly efficient for handling categorical data because it can process such features automatically without requiring manual encoding\n",
        "\n",
        "\n",
        "**2** In traditional models, categorical data must be converted into numbers using Label Encoding or One-Hot Encoding, which can increase complexity and memory usage\n",
        "\n",
        "\n",
        "**3** CatBoost uses a special technique called Target Encoding with Ordered Statistics, which replaces categorical values with numerical representations based on their relationship with the target variable while avoiding data leakage\n",
        "\n",
        "\n",
        "**4** It calculates these encodings in an ordered way — using only the information from previous rows — to ensure the model doesn’t use future data during training\n",
        "\n",
        "\n",
        "**5**  This built-in handling of categorical variables makes CatBoost faster and easier to use compared to other boosting algorithms like XGBoost and LightGBM\n",
        "\n",
        "\n",
        "**6** CatBoost also supports efficient encoding even when there are high-cardinality categorical features (i.e., features with many unique categories)\n",
        "\n",
        "\n",
        "**7**  It automatically detects categorical columns, processes them internally, and reduces the need for manual preprocessing steps\n",
        "\n",
        "\n",
        "**8**  This not only saves time but also improves the accuracy of the model, since it captures category-target relationships more effectively\n",
        "\n",
        "\n",
        "**9**  CatBoost’s name itself comes from “Categorical Boosting,” highlighting its strong capability for categorical feature handling\n",
        "\n",
        "\n",
        "**10** Hence, CatBoost is considered efficient because it simplifies preprocessing, prevents overfitting from manual encodings, and provides accurate and stable results for datasets containing categorical data\n",
        "\n"
      ],
      "metadata": {
        "id": "m-_IntyJMdBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question** **5**: What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?"
      ],
      "metadata": {
        "id": "SdlY3B8XM-k0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1** Boosting techniques are preferred over bagging methods in applications where high accuracy and low bias are important\n",
        "\n",
        "\n",
        "**2** They are commonly used in financial risk modeling, such as credit scoring and fraud detection, where small improvements in prediction accuracy can have large impacts\n",
        "\n",
        "\n",
        "**3** In marketing and sales prediction, boosting helps identify potential customers, predict customer churn, and improve targeted marketing campaigns\n",
        "\n",
        "\n",
        "**4** In healthcare, boosting algorithms are used for disease prediction, patient readmission risk analysis, and medical image classification\n",
        "\n",
        "\n",
        "**5** In cybersecurity, boosting is applied for detecting spam emails, malware, and network intrusions with higher accuracy\n",
        "\n",
        "\n",
        "**6**  In e-commerce, boosting models power product recommendation systems and dynamic pricing strategies\n",
        "\n",
        "**7**  In insurance, it helps in claim prediction, customer segmentation, and risk assessment\n",
        "\n",
        "\n",
        "**8**  In text classification and sentiment analysis, boosting techniques like XGBoost and CatBoost improve accuracy in understanding opinions or categorizing documents\n",
        "\n",
        "\n",
        "**9**  In manufacturing and quality control, boosting detects faulty products and predicts machine failures through sensor data analysis\n",
        "\n",
        "\n",
        "**10** Overall, boosting methods are preferred when the goal is to reduce bias, improve prediction accuracy, and capture complex relationships that simple or bagging models may miss\n",
        "\n"
      ],
      "metadata": {
        "id": "PtZ0xqV9OAoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Question** **6**:  Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n",
        "(Include your Python code and output in the code box below.)?"
      ],
      "metadata": {
        "id": "PF0L1BzQPE4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and check accuracy\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, model.predict(X_test)))"
      ],
      "metadata": {
        "id": "1bm9j1DeQQOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question** **7**: Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score?"
      ],
      "metadata": {
        "id": "q-jBo1-NQ0mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# loading dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# splitting data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# training Gradient Boosting model\n",
        "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predicting and checking performance\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "kMSWVOfKRLEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question** **8**: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy?"
      ],
      "metadata": {
        "id": "RPb1-03jRSG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "params = {'learning_rate': [0.01, 0.05, 0.1, 0.2]}\n",
        "\n",
        "grid = GridSearchCV(model, params, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, grid.best_estimator_.predict(X_test)))"
      ],
      "metadata": {
        "id": "8YbbZdy0Rhh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY-jXMzYI-wL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6dpfUL-Ikxh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eEj6oPhI9GX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YfzaxBHmIlY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-aiKMozxIlb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qqucq3HTIlfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zUeK0YANIliF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XyOG4kTMIlky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BZMTbXhGIlnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3eNigPP1IlqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ACvPdExxIltE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GX2R_t54Ilyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_e5U3HdIl1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zghPj2FIl4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eG3L_c6lIl7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xp0q3Is2Il-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hfxz7UXSImBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZcdGz7EEImED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1oLfGUQImJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o19TKJl-ImMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ALB-AmhIImTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pu9EsPhSImWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ge8w0L9oImZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qthqcKdnImcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWngwkxtImfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oDfDW3FImiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RxylRfCImlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVFgk4LMImoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26HHKmhKImrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yTBwmdtAImuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HUbyMX9YImxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EXxi8KypIm0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8s6okQ3Im20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "27wIJWE9Im5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "osaqrK8xIm8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "umRRtdCAIm_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "730EuTseInHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgvDju25InKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Qg3w27sInNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WHGxTlpvInQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWbljmCDInTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A_9myrPuInWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZjP6nFVInZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EbQbSuuXIncD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eav9qGKDIri0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sg8cCLVZIrrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dyzhA_l_Irtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1DyhOMUjIrv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hw0pZ7ZBIrye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}